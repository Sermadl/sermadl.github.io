---
title: "ìƒì„±í˜• AI ë¯¸ë‹ˆ í”„ë¡œì íŠ¸(1): ì‹¤ì‹œê°„ STT, ë²ˆì—­ (ì´ˆê¸° API ì—°ê²°)"
categories: [Fast API]
tags: [ML, Fast API, STT, Open AI API, SSE, SKALA, SKALA1ê¸°, SK]
---

> Skala ê³¼ì •ì—ì„œ ìƒì„±í˜• AI ëŒ€í•´ ìƒˆë¡­ê²Œ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.<br>
> ìƒì„±í˜• AI ìˆ˜ì—… ì¤‘ ì§„í–‰í•˜ê²Œ ëœ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸ êµ¬í˜„ ê³¼ì •ì„ ê¸°ìˆ í•©ë‹ˆë‹¤.

<hr>

êµìˆ˜ë‹˜ê»˜ì„œ "**ì‹¤ì‹œê°„**" "**ìŒì„±**" ê¸°ë°˜ "**B2B**" ì¸ê³µì§€ëŠ¥ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸ë¥¼ í•œ ë²ˆ ë§Œë“¤ì–´ë³´ë¼ê³  í•˜ì…”ì„œ<br>
ì €í¬ ì¡°ëŠ” í•´ì™¸ ì¶œì¥ì´ ì¦ì€ íšŒì‚¬ë¥¼ ë‹¤ë‹ˆëŠ” íšŒì‚¬ì›ë“¤ì„ ìœ„í•´ ì‹¤ì‹œê°„ ë²ˆì—­ ë° OCR ê¸°ë°˜ ì¶œì¥ ê²½ë¹„ ë³´ê³ ì„œ ì‘ì„± ëª¨ë¸ì„ ì„œë¹™í•˜ëŠ” í”Œë«í¼ì„ ì œì‘í•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.<br>

## ì´ˆê¸° êµ¬ìƒ

í° ê¸°ëŠ¥ì€

1. ì‹¤ì‹œê°„ ì–‘ë°©í–¥ ë²ˆì—­
2. ì˜ìˆ˜ì¦ ê¸°ë°˜ ì¶œì¥ ë³´ê³ ì„œ ì‘ì„±

ì…ë‹ˆë‹¤.<br>

ì € ë‘ ê°œë§Œ í•˜ë”ë¼ë„ ì•½ 10ì¼,,ì •ë„ì˜ ì‹œê°„ì´ ì£¼ì–´ì¡Œê¸° ë•Œë¬¸ì— êµ‰ì¥íˆ ì´‰ë°•í•  ê²ƒ ê°™ì•„ì„œ ê¸°ëŠ¥ì„ ì¶•ì†Œí–ˆìŠµë‹ˆë‹¤.<br>

ì¡°ì›ë“¤ê³¼ í•¨ê»˜ ê¸°ëŠ¥ëª…ì„¸ì„œ ë° API ëª…ì„¸ì„œ, ì™€ì´ì–´í”„ë ˆì„ë„ ì‘ì„±í–ˆìŠµë‹ˆë‹¤ ^-^<br>

ì´ ì¤‘ ì œê°€ ë§¡ì€ ì—­í• ì€ ì‹¤ì‹œê°„ ì–‘ë°©í–¥ ë²ˆì—­ ê¸°ëŠ¥ ë° Fast API(ë°±ì—”ë“œ) êµ¬ì„±ì…ë‹ˆë‹¤!<br>

## ì‹¤ì‹œê°„ STT ê¸°ëŠ¥ ì´ˆê¸° êµ¬ì„±í•˜ê¸°

Googleì˜ Speech-To-Text APIë¥¼ ì‚¬ìš©í•´ì„œ STT ê¸°ëŠ¥ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.<br>

ì•„ì§ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŒì„±ì„ ë°›ì•„ì˜¤ëŠ” ë°©ì‹ì— ëŒ€í•´ ë…¼ì˜ë¥¼ ë” í•´ë´ì•¼ í•  ê²ƒ ê°™ì•„ì„œ<br>
ìŒì„±ì„ ë°˜í™˜í•˜ëŠ” ë°©ì‹ë§Œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì§„í–‰ë  ìˆ˜ ìˆë„ë¡ Event Streamì„ í†µí•´ ê²°ê³¼ê°’ì„ ë°˜í™˜í•˜ë„ë¡ êµ¬í˜„í•´ë‘ì—ˆìŠµë‹ˆë‹¤<br>

<hr>

### Google Speech-To-Text API ì‚¬ìš© ì„¤ì •

ì‚¬ì‹¤ ì´ ê³¼ì •ì´ ê°€ì¥ í˜ë“¤ì—ˆìŠµë‹ˆë‹¤ ^.^<br>

ì˜¤ë¡œì§€ [ê³µì‹ ë¬¸ì„œ](https://cloud.google.com/speech-to-text/v2/docs?hl=ko)ë¥¼ ë³´ê³  êµ¬í˜„ì„ ì™„ë£Œí–ˆëŠ”ë°,<br>

ê°€ì´ë“œ ê¸°ëŠ¥ì„ ì§€ì›í•´ì£¼ì–´ì„œ êµ‰ì¥íˆ ì‰½ê³  í¸ì•ˆí•˜ê²Œ API ì‚¬ìš© ì„¤ì •ì„ ì™„ë£Œí•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤!<br>

ìŠ¤íŠ¸ë¦¬ë° ì…ë ¥ì˜ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ë„ ìˆì—ˆì§€ë§Œ,<br>
í”„ë¡ íŠ¸ì—ì„œ ìŒì„±ì„ ë°±ì—”ë“œë¡œ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì „ë‹¬í• ì§€ ì•„ì§ ì •í•´ì§„ê²Œ ì—†ì–´ì„œ<br>

ì¼ë‹¨ì€ ìŒì„± íŒŒì¼ì„ ì…ë ¥í•˜ê³  ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ê²°ê³¼ ê°’ì„ ë°˜í™˜ë°›ë„ë¡ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.<br>

ì €ëŠ” [ì´ ë¬¸ì„œ](https://cloud.google.com/speech-to-text/v2/docs/sync-recognize?hl=ko)ë¥¼ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤.<br>

ì´ˆê¸°ì— í™˜ê²½ êµ¬ì„±ì„ í•˜ëŠ” ë°©ë²•ì€ [íŠœí† ë¦¬ì–¼ ê¸°ëŠ¥](https://cloud.google.com/speech-to-text/v2/docs/console-tutorials?hl=ko)ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.<br>

ì•„ë§ˆ `gcloud` ê¹Œì§€ ì—°ë™ì„ í•´ì•¼ë§Œ STT ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²ƒ ê°™ì•„ì„œ<br>
ê°€ì´ë“œë¥¼ ë”°ë¼ gcloudë„ ì„¤ì¹˜í–ˆìŠµë‹ˆë‹¤.<br>

<hr>

### .py íŒŒì¼ë¡œ ì—°ê²° í…ŒìŠ¤íŠ¸í•˜ê¸°

ë¨¼ì € ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”!

```shell
pip install google-cloud-speech python-dotenv gtts
```

<br>

ê·¸ ë‹¤ìŒ `json` íŒŒì¼ë¡œ ë‹¤ìš´ë°›ì„ ìˆ˜ ìˆëŠ” API Keyë¥¼ ì´ìš©í•´ì„œ ì‹¤ì œë¡œ ì—°ê²°ì´ ë˜ëŠ”ì§€ í™•ì¸ì„ í•´ë³´ê² ìŠµë‹ˆë‹¤.<br>

ê³µì‹ ë¬¸ì„œì— ìˆë˜ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ë³µë¶™í•´ì„œ ëŒë¦¬ë ¤ëŠ”ë° ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ì–´ì„œ...<br>

[ gtts.py ]

```python
from gtts import gTTS
import os

# í…ìŠ¤íŠ¸ ì •ì˜
text = "The learning principle of an AI model is a process of adjusting internal weights using input data so that it can produce the desired output."

# TTSë¡œ MP3 ìƒì„±
tts = gTTS(text)
tts.save("./tts/ai_learning.mp3")
```

ì´ë ‡ê²Œ tts ì½”ë“œë¥¼ ì‘ì„±í•´ì„œ ìŒì„± íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤ ã…ã…<br>

<hr>

[ google-stt-stream.py ]

```python
import os
from dotenv import load_dotenv

from google.cloud.speech_v2 import SpeechClient
from google.cloud.speech_v2.types import cloud_speech as cloud_speech_types

load_dotenv()
project_id = os.getenv("PROJECT_ID")
api_key = os.getenv("API_KEY")

def transcribe_streaming_v2(
    stream_file: str,
) -> cloud_speech_types.StreamingRecognizeResponse:
    """Transcribes audio from an audio file stream using Google Cloud Speech-to-Text API.
    Args:
        stream_file (str): Path to the local audio file to be transcribed.
            Example: "resources/audio.wav"
    Returns:
        list[cloud_speech_types.StreamingRecognizeResponse]: A list of objects.
            Each response includes the transcription results for the corresponding audio segment.
    """
    # Instantiates a client
    client = SpeechClient()

    # Reads a file as bytes
    with open(stream_file, "rb") as f:
        audio_content = f.read()

    # In practice, stream should be a generator yielding chunks of audio data
    chunk_length = len(audio_content) // 5
    stream = [
        audio_content[start : start + chunk_length]
        for start in range(0, len(audio_content), chunk_length)
    ]
    audio_requests = (
        cloud_speech_types.StreamingRecognizeRequest(audio=audio) for audio in stream
    )

    recognition_config = cloud_speech_types.RecognitionConfig(
        auto_decoding_config=cloud_speech_types.AutoDetectDecodingConfig(),
        language_codes=["en-US"],
        model="long",
    )
    streaming_config = cloud_speech_types.StreamingRecognitionConfig(
        config=recognition_config
    )
    config_request = cloud_speech_types.StreamingRecognizeRequest(
        recognizer=f"projects/{project_id}/locations/global/recognizers/_",
        streaming_config=streaming_config,
    )

    def requests(config: cloud_speech_types.RecognitionConfig, audio: list) -> list:
        yield config
        yield from audio

    # Transcribes the audio into text
    responses_iterator = client.streaming_recognize(
        requests=requests(config_request, audio_requests)
    )
    responses = []
    for response in responses_iterator:
        responses.append(response)
        for result in response.results:
            print(f"Transcript: {result.alternatives[0].transcript}")

    return responses

transcribe_streaming_v2("./tts/ai_learning.mp3")
```

í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê³µì‹ ë¬¸ì„œì—ì„œ ë³µë¶™í•œ ì½”ë“œì…ë‹ˆë‹¤. (í™˜ê²½ ë³€ìˆ˜ë¥¼ ìœ„í•´ ì œì¼ ìœ—ë¶€ë¶„ì„ ìˆ˜ì •í•˜ê³ , ì‹¤ì œ ì‹¤í–‰ì„ ìœ„í•´ ê°€ì¥ ì•„ë« ë¶€ë¶„ë§Œ ì‚´ì§ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤)<br>

ìƒìœ„ì— ìˆëŠ”

```python
load_dotenv()
project_id = os.getenv("PROJECT_ID")
api_key = os.getenv("API_KEY")
```

ë¥¼ í†µí•´ í™˜ê²½ë³€ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.<br>

> í•´ë‹¹ ì½”ë“œë¥¼ ì‘ì„±í•œ íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì— `.env` ë¼ëŠ” ì´ë¦„ì˜ íŒŒì¼ì— í™˜ê²½ë³€ìˆ˜ë¥¼ ì‘ì„±í•´ì•¼ ì½”ë“œê°€ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
> <br><br>Ex: `PROJECT_ID=project_id`

<hr>

### ê²°ê³¼ í™”ë©´

![stt-test](/assets/img/stt-test.png)

ì•„ì£¼ ì˜ ë‚˜ì˜µë‹ˆë‹¹ ã….ã…<br>

ì¡°ì› ë¶„ë“¤ê³¼ì˜ ìƒì˜ë¥¼ í†µí•´ ì¶”í›„ì— ê°ì§€í•  ì–¸ì–´ë¥¼ ì„ íƒí•˜ëŠ” ë¡œì§ë„ ì¶”ê°€ë  ì˜ˆì •ì…ë‹ˆë‹¤<br>

<hr>

## ì‹¤ì‹œê°„ Open AI ë‹µë³€ ê¸°ëŠ¥ ì´ˆê¸° êµ¬ì„±í•˜ê¸°

Open AI APIë¥¼ í†µí•´ì„œ ì–¸ì–´ ë²ˆì—­ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ì˜ˆì •ì…ë‹ˆë‹¤.<br>

ê·¸ëƒ¥ ë²ˆì—­ì„ í•´ì£¼ëŠ” ë°ì—ì„œ ê·¸ì¹˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë¹„ì¦ˆë‹ˆìŠ¤ì ì¸ ë§ì´ ì˜¤ê³ ê°ˆ ê²ƒì„ ì˜ˆìƒí•´ì„œ<br>
ì¿ ì…˜ì–´ë¥¼ ì ìš©í•œë‹¤ë˜ì§€ ì‹¤ì œë¡œ í†µë²ˆì—­ì‚¬ ë¶„ë“¤ì´ ë²ˆì—­ì„ í•˜ì‹¤ ë•Œ ì–¸ì–´ë¥¼ ìˆœí™”í•´ì£¼ëŠ” ê²ƒì„ ëª¨ë°©í•´ë³´ê³ ì‹¶ì–´ì„œ<br>
Open AIë¥¼ ë²ˆì—­ ëª¨ë¸ë¡œ ì‚¬ìš©í•´ë³´ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.<br>

<hr>

### .pyë¡œ ì—°ê²° í…ŒìŠ¤íŠ¸í•˜ê¸°

ì•„ê¹Œ STT í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ìƒì„±í–ˆë˜ ë””ë ‰í† ë¦¬ì™€ ê°™ì€ ë””ë ‰í† ë¦¬ì— ì½”ë“œë¥¼ ì‘ì„±í•´ì„œ<br>
.env íŒŒì¼ì„ ë‘ ë²ˆ ë§Œë“¤ì§€ ì•Šê³  í™˜ê²½ë³€ìˆ˜ë¥¼ ë“±ë¡í•´ ë‘ì—ˆìŠµë‹ˆë‹¤.<br>

[open-ai-stram.py]

```python
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_teddynote.messages import stream_response
from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI(model="gpt-4o", temperature=0.5, max_tokens=1024)
templete='{text}ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë²ˆì—­ ëœ ë¬¸ì¥ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.'
prompt = PromptTemplate.from_template(templete)
output_parser = StrOutputParser()
chain = prompt | model | output_parser

def get_streaming_message_from_openai(data: str):
    try:
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        response = chain.stream(data)
        # stream_response(response)

        for token in response:
            print(f"Token: {token}")
        print("data: [DONE]\n\n")
    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
        print(f"[Error: {str(e)}]")

get_streaming_message_from_openai("The learning principle of an AI model is a process of adjusting internal weights using input data so that it can produce the desired output.")
```

ì•„ê¹Œ ì¶”ì¶œí•œ ë¬¸ì¥ì„ ë²ˆì—­í•´ë³´ê² ìŠµë‹ˆë‹¤.<br>

ì½”ë“œëŠ” Skala ìƒì„±í˜• AI ê³¼ì • ì¤‘ êµìˆ˜ë‹˜ê»˜ ë°›ì€ ì½”ë“œ ì¡°ê°ë“¤ì„ ëª¨ì•„ì„œ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤..!!<br>

> ê²°ì½” ì‰½ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ~~ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì•ˆ ë§ê³  ì–´ì©Œê³ ..~~

chainì„ ì‚¬ìš©í•´ì„œ input, prompt, model, outputì„ ì—®ì–´ì£¼ì—ˆê³ <br>
output parserë¥¼ ì´ìš©í•´ì„œ ê²°ê³¼ê°’ë§Œ ë°›ì•„ì˜¬ ìˆ˜ ìˆë„ë¡ í–ˆìŠµë‹ˆë‹¤.<br>

<hr>

### ê²°ê³¼ í™”ë©´

![ai-stream-test](/assets/img/ai-stream-test.png)

ì§œì”~<br>

<hr>

## Fast API ì„œë²„ì— ì ìš©í•´ë³´ê¸°

![fast-api-directory](/assets/img/fast-api-directory.png)

ìœ„ ì‚¬ì§„ì´ ì œê°€ êµ¬ì„±í•œ Fast API ì„œë²„ì˜ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì…ë‹ˆë‹¤.<br>

ì¡°ì›ë¶„ë“¤ê³¼ì˜ ì›í™œí•œ ì½”ë“œ ê³µìœ (?)ë¥¼ ìœ„í•´ `requiremnets.txt` íŒŒì¼ë„ ì‘ì„±í•´ë‘ì—ˆìŠµë‹ˆë‹¤.<br>

> í”„ë¡ íŠ¸ì™€ì˜ ì†Œí†µ ë°©ì‹ì— ì›¹ì†Œì¼“ì„ ì ìš©í• ì§€ ì•„ì§ ë…¼ì˜ì¤‘ì´ë¼ ì½”ë“œê°€ SSE ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¹<br>

<hr>

### main.py

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.api import stt, openai

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(stt.router, prefix="/stt", tags=["Google STT"])
app.include_router(openai.router, prefix="/openai", tags=["OpenAI"])

@app.get("/")
def read_root():
    return {"message": "Welcome to the FastAPI Google Speech-to-Text service!"}
```

`main.py` ì—ëŠ” cors ì„¤ì •ê³¼ ë¼ìš°í„° ì„¤ì •ë§Œ ì‘ì„±í•´ë‘ì—ˆìŠµë‹ˆë‹¤.<br>

<hr>

### openai.py

```python
from fastapi import APIRouter, Request, status
from fastapi.responses import StreamingResponse
from app.services.openai_service import get_streaming_message_from_openai
from app.models.message_request import MessageRequest

router = APIRouter()

@router.post(
    "/streaming/",
    summary="Generate messages using OpenAI (streaming)",
)
async def get_generated_messages_streaming(
    data: MessageRequest,
):
    """
    ## Send a prompt to OpenAI
    - message: str
    """
    print("Streaming started")
    return StreamingResponse(
        get_streaming_message_from_openai(data), media_type="text/event-stream"
    )
```

StreamingResponseë¥¼ í†µí•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²°ê³¼ê°’ì„ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •í•´ë‘ì—ˆìŠµë‹ˆë‹¤.<br>

<hr>

### stt.py

```python
from fastapi import APIRouter, UploadFile, File
from fastapi.responses import StreamingResponse
from app.services.google_stt import transcribe_streaming_v2

router = APIRouter()

@router.post("/transcribe/")
async def transcribe_audio(file: UploadFile = File(...)):
    try:
        print(f"File type: {type(file)}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
        print(f"File name: {file.filename}")  # íŒŒì¼ ì´ë¦„ í™•ì¸


        audio_content = await file.read()  # íŒŒì¼ ë‚´ìš©ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì½ìŒ

        # StreamingResponseë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ë°˜í™˜
        return StreamingResponse(
            transcribe_streaming_v2(audio_content), media_type="text/event-stream"
        )

    except Exception as e:
        return StreamingResponse(
            (f"data: [Error: {str(e)}]\n\n" for _ in range(1)),  # ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°˜í™˜
            media_type="text/event-stream",
            status_code=500,
        )
```

STT ì—­ì‹œ StreamingResponseë¥¼ í†µí•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²°ê³¼ ê°’ì„ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.<br>

`MessageRequest` ëŠ” DTO ì…ë‹ˆë‹¤!<br>

[ message_request.py ]

```python
from pydantic import BaseModel

class MessageRequest(BaseModel):
    message: str
```

<hr>

### google_stt.py

```python
import os
from dotenv import load_dotenv
from google.cloud.speech_v2 import SpeechClient
from google.cloud.speech_v2.types import cloud_speech as cloud_speech_types

load_dotenv()

PROJECT_ID = os.getenv("PROJECT_ID")

async def transcribe_streaming_v2(audio_content: bytes):
    client = SpeechClient()

    # ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ”
    chunk_length = len(audio_content) // 5
    stream = [
        audio_content[start : start + chunk_length]
        for start in range(0, len(audio_content), chunk_length)
    ]
    audio_requests = (
        cloud_speech_types.StreamingRecognizeRequest(audio=audio) for audio in stream
    )

    recognition_config = cloud_speech_types.RecognitionConfig(
        auto_decoding_config=cloud_speech_types.AutoDetectDecodingConfig(),
        language_codes=["en-US"],
        model="long",
    )
    streaming_config = cloud_speech_types.StreamingRecognitionConfig(
        config=recognition_config
    )
    config_request = cloud_speech_types.StreamingRecognizeRequest(
        recognizer=f"projects/{PROJECT_ID}/locations/global/recognizers/_",
        streaming_config=streaming_config,
    )

    def requests(config: cloud_speech_types.RecognitionConfig, audio: list):
        yield config
        yield from audio

    # Google STT API í˜¸ì¶œ
    responses_iterator = client.streaming_recognize(
        requests=requests(config_request, audio_requests)
    )

    # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ë°˜í™˜
    for response in responses_iterator:
        for result in response.results:
            transcript = result.alternatives[0].transcript
            yield f"data: {transcript}\n\n"  # EventStream í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
```

asyncê³¼ yieldë¥¼ ì ì ˆí•˜ê²Œ ì‚¬ìš©í•´ì„œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.<br>

returnì„ ì‚¬ìš©í•˜ë©´ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ì—†ì–´ì„œ<br>
yieldë¥¼ ì‚¬ìš©í•´ì„œ í† í°ì´ ìƒˆë¡œ ì˜¬ ë•Œë§ˆë‹¤ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •í•´ë‘ì—ˆìŠµë‹ˆë‹¤!([returnê³¼ yield](https://www.daleseo.com/python-yield/))<br>

<hr>

### openai_service.py

```python
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_teddynote.messages import stream_response
from dotenv import load_dotenv
from app.models.message_request import MessageRequest

load_dotenv()

model = ChatOpenAI(model="gpt-4o-mini", temperature=0.5, max_tokens=1024)
templete='{text}ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë²ˆì—­ ëœ ë¬¸ì¥ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.'
prompt = PromptTemplate.from_template(templete)
output_parser = StrOutputParser()
chain = prompt | model | output_parser

async def get_streaming_message_from_openai(data: MessageRequest):
    try:
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        response = chain.stream(data.message)
        # stream_response(response)

        for token in response:
            print(f"Token: {token}") # ë””ë²„ê¹… ìš© ì½”ë“œì…ë‹ˆë‹¤
            content = f"data: {token}\n\n"
            yield content
        yield "data: [DONE]\n\n"
    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
        yield f"[Error: {str(e)}]"
```

ì´ ì½”ë“œë„ ë§ˆì°¬ê°€ì§€ë¡œ ìƒˆë¡œìš´ í† í°ì´ ìƒì„±ë  ë•Œë§ˆë‹¤ yieldë¡œ ë°˜í™˜ë  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•´ë‘ì—ˆìŠµë‹ˆë‹¤.<br>

<hr>

### ê²°ê³¼ í™”ë©´

![fast-api-swagger](/assets/img/fast-api-swagger.png)

<hr>

![stt-swagger-test](/assets/img/stt-swagger-test.png)

<hr>

![open-ai-swagger-test](/assets/img/open-ai-swagger-test.png)

<br>

ì§œì” ^.^~~<br>
ì„±ê³µì…ë‹ˆë‹¹ ã….ã…..<br>

<hr>

## ë§ˆì¹˜ë©°

ì•„ë¬´ë˜ë„ ì‹¤ì‹œê°„ ì–‘ë°©í–¥ ë²ˆì—­ì´ê¸° ë•Œë¬¸ì— ì›¹ì†Œì¼“ì˜ ì ìš©ì´ ë¶ˆê°€í”¼í•  ê²ƒ ê°™ê¸´í•˜ì§€ë§Œ...<br>
ìŒì„±ì„ ì…ë ¥ë°›ì•„ì•¼í•˜ê¸° ë•Œë¬¸ì— ì •ë§ì •ë§ ë¶ˆê°€í”¼í•  ê²ƒ ê°™ì§€ë§Œ..<br>

ê·¸ë˜ë„ ì¼ë‹¨ì€ SSE ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ì„ í•´ë‘ì—ˆìŠµë‹ˆë‹¤!<br>

ì˜¬ë°”ë¥¸ êµ¬ì¡°ì— ëŒ€í•œ ê³ ë¯¼ì´ ê¹Šì–´ì§€ëŠ”,,í”„ë¡œì íŠ¸,,,ì…ë‹ˆë‹¤,,,,,<br>

ğŸ¤”ğŸ’­

<hr>
<br>

> ì°¸ê³  ìë£Œ
> [í•˜ë‚˜](https://cloud.google.com/speech-to-text?hl=ko), [ë‘˜](https://suloth.tistory.com/127), Skala ìˆ˜ì—…ìë£Œ!
